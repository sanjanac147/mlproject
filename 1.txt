#1a
import re

def extract_data_from_file(file_path, pattern):
    try:
        with open(file_path, 'r') as file:
            data = file.read()

            matches = re.findall(pattern, data)
            return matches
    except FileNotFoundError:
        print(f"Error: File '{file_path}' not found.")
    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage:
file_path = 'un.txt'  # Replace with the path to your text file
pattern = r"\d{2}[/-]\d{2}[/-]\d{4}"  # Example pattern to extract a social security number

result = extract_data_from_file(file_path, pattern)

if result:
    print(f"Extracted data: {result}")
else:
    print("No matching data found.")

#1b
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer


def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Stop Word Removal
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token.lower() not in stop_words]
    print(tokens)
    # Stemming using Porter Stemmer
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(token) for token in tokens]

    return tokens

# Example usage:
input_text = "Text the preprocessing a involves tokenization, stop word removal, and stemming."

preprocessed_tokens = preprocess_text(input_text)

print("Original Text:")
print(input_text)

print("\nPreprocessed Tokens:")
print(preprocessed_tokens)

#2b
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import ngrams


def generate_ngrams(text, n):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]

    n_grams = list(ngrams(words, n))
    return n_grams

# Example usage:
input_text = "This is an example sentence for N-Gram modeling. We want to exclude common stop words."

n_value = 2  # You can change this to generate different N-Grams (e.g., 2 for bigrams, 3 for trigrams, etc.)

result_ngrams = generate_ngrams(input_text, n_value)

print(f"{n_value}-Grams:")
for n_gram in result_ngrams:
    print(n_gram)

#3a
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import matplotlib.pyplot as plt

def plot_most_frequent_word(text):
    # Tokenize the text
    words = word_tokenize(text)

    # Calculate word frequencies
    freq_dist = FreqDist(words)

    # Get the most common word
    most_common_word = freq_dist.most_common(10)
    print(most_common_word)
    # Plot the frequency distribution
    freq_dist.plot(30, cumulative=False)


# Example usage:
input_text = "This is an example sentence This . It contains repeated This words to demonstrate the This word frequency distribution."

plot_most_frequent_word(input_text)

#3b
import re
def test(string):
  merged = re.split(r"([ ,!]+)", string)
  print(merged)
  return [merged[::2], merged[1::2]]
s = input("Enter a string with , separators")
print("\nOriginal string:",s)
print("Split the said string into 2 lists: words and separators:")
print(test(s))

#4a
import re

def sum_numbers_from_file(file_path):
    try:
        with open(file_path, 'r') as file:
            content = file.read()

        numbers = [int(match) for match in re.findall(r'\d+', content)]
        total_sum = sum(numbers)

        return total_sum

    except FileNotFoundError:
        print(f"Error: File '{file_path}' not found.")
        return None

# Example usage:
file_path = 'un.txt'  # Replace with the path to your file

result = sum_numbers_from_file(file_path)

if result is not None:
    print(f"Sum of numbers in the file: {result}")

#4b
import re

def extract_capital_words(sentence):
    pattern = r'\b[A-Z][a-z]*\b'
    matches = re.findall(pattern, sentence)
    return matches

# Example usage:
sample_sentence = "This is a Sample sentence with Some Capitalized Words like Python, Regular Expressions, and Programming."
result = extract_capital_words(sample_sentence)
print(result)

#5b

import nltk
text = "There is a man on the hill, and I watched him with my telescope. "
tokens = nltk.word_tokenize(text)
print(tokens)
tag = nltk.pos_tag(tokens)
print(tag)
grammar = "NP: {<DT>?<JJ>*<NN>}"
cp =nltk.RegexpParser(grammar)
result = cp.parse(tag)
print(result)
result.draw()

#6a
import re

def extract_twitter_handles(text):
    # Define the regular expression pattern for a Twitter handle
    pattern = r'https://twitter\.com/([a-zA-Z0-9_]+)'

    # Use re.findall to find all matches in the text
    handles = re.findall(pattern, text)

    return handles

# Example usage
input_text = "Check out my Twitter profile: https://twitter.com/username1. Also, follow https://twitter.com/another_user_123 for updates."
twitter_handles = extract_twitter_handles(input_text)

print("Twitter handles found:", twitter_handles)

#6b
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import string

def plot_word_distribution(text):
    # Tokenize the text
    words = word_tokenize(text)

    # Remove punctuation and convert to lowercase
    words = [word.lower() for word in words if word.isalpha()]

    # Remove stop words
    stop_words = set(stopwords.words("english"))
    words = [word for word in words if word not in stop_words]

    # Count word frequencies
    word_freq = Counter(words)

    # Get the most common words and their frequencies
    most_common_words = word_freq.most_common(10)

    # Plot the word distribution
    plt.bar([word[0] for word in most_common_words], [freq[1] for freq in most_common_words])
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.title('Top 10 Most Frequently Distributed Words')
    plt.show()

if __name__ == "__main__":
    # Replace this string with your own text
    sample_text = "Write your sample text here. This is just an example text for plotting word distribution."

    plot_word_distribution(sample_text)

