#7
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk.stem import WordNetLemmatizer

# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('wordnet')

def perform_lemmatization(text):
    # Tokenize the text into words
    words = word_tokenize(text)

    # Perform part-of-speech tagging
    pos_tags = pos_tag(words)

    # Initialize lemmatizer
    lemmatizer = WordNetLemmatizer()

    # Map Penn Treebank POS tags to WordNet POS tags
    pos_mapping = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}

    # Lemmatize words based on their POS tags
    lemmatized_words = []
    for word, pos in pos_tags:
        pos = pos[0].upper()
        pos = pos_mapping.get(pos, 'n')  # Default to noun if not found
        lemmatized_word = lemmatizer.lemmatize(word, pos)
        lemmatized_words.append(lemmatized_word)

    return lemmatized_words

# Example text
text = "The quick brown foxes are jumping over the lazy dogs."

# Perform lemmatization based on parts of speech
lemmatized_text = perform_lemmatization(text)

# Print the result
print("Original Text:")
print(text)
print("\nLemmatized Text:")
print(" ".join(lemmatized_text))

-
#8--------------------
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer
# define an method
def morphological_analysis(sentence):
  words = word_tokenize(sentence)
# Porter Stemmer
  porter_stemmer = PorterStemmer()
  porter_stems = [porter_stemmer.stem(word) for word in words]
# Lancaster Stemmer
  lancaster_stemmer = LancasterStemmer()
  lancaster_stems = [lancaster_stemmer.stem(word) for word in words]
# Snowball Stemmer
  snowball_stemmer = SnowballStemmer('english')
  snowball_stems = [snowball_stemmer.stem(word) for word in words]
# WordNet Lemmatizer
  lemmatizer = WordNetLemmatizer()
  lemmas = [lemmatizer.lemmatize(word) for word in words]
  print("Original Sentence:", sentence)
  print("\nPorter Stems:", porter_stems)
  print("Lancaster Stems:", lancaster_stems)
  print("Snowball Stems:", snowball_stems)
  print("WordNet Lemmas:", lemmas)

sentence = "Morphological analysis is important for natural language processing."
morphological_analysis(sentence)

#9----------------------
import nltk
from nltk import bigrams
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist

# Sample collection of three documents
documents = [
    "This is the first document.",
    "The second document is here.",
    "And this is the third document."
]

def find_bi_gram_frequency(documents):
    # Tokenize and get bigrams for each document
    bi_grams_list = [list(bigrams(word_tokenize(doc.lower()))) for doc in documents]

    # Flatten the list of bigrams
    all_bi_grams = [bi_gram for sublist in bi_grams_list for bi_gram in sublist]

    # Calculate the frequency distribution of bigrams
    bi_gram_freq = FreqDist(all_bi_grams)

    # Display the number of unique bigrams
    print(f"Number of unique bi-grams: {len(bi_gram_freq)}")

    # Display the top 5 most common bigrams
    print("\nTop 5 most common bi-grams:")
    for bi_gram, frequency in bi_gram_freq.most_common(5):
        print(f"{bi_gram}: {frequency}")

# Find and display bi-gram frequency
find_bi_gram_frequency(documents)

#10----------------

non_terminals = ["NP", "Nom", "Det", "AP", "Adv", "A"]
terminals = ["book", "orange", "man", "tall", "heavy", "very", "muscular"]
R = {
    "NP": [["Det", "Nom"]],
    "Nom": [["AP", "Nom"], ["book"], ["orange"], ["man"]],
    "AP": [["Adv", "A"], ["heavy"], ["orange"], ["tall"]],
    "Det": [["a"]],
    "Adv": [["very"], ["extremely"]],
    "A": [["heavy"], ["orange"], ["tall"], ["muscular"]]
}

def cykParse(w):
    n = len(w)
    T = [[set([]) for _ in range(n)] for _ in range(n)]

    for j in range(n):
        for lhs, rule in R.items():
            for rhs in rule:
                if len(rhs) == 1 and rhs[0] == w[j]:
                    T[j][j].add(lhs)

        for i in range(j, -1, -1):
            for k in range(i, j + 1):
                for lhs, rule in R.items():
                    for rhs in rule:
                        if len(rhs) == 2 and rhs[0] in T[i][k] and rhs[1] in T[k + 1][j]:
                            T[i][j].add(lhs)

    if len(T[0][n - 1]) != 0:
        print("True")
    else:
        print("False")

w = "a very tall red man".split()
cykParse(w)

#11--------------------------
def min_edit_distance(source, target):
    m, n = len(source), len(target)

    # Initialize a matrix to store the edit distances
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    print(dp)
    # Initialize the first row and column
    for i in range(m + 1):
        dp[i][0] = i
    for j in range(n + 1):
        dp[0][j] = j

    # Fill in the matrix using dynamic programming
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            cost = 0 if source[i - 1] == target[j - 1] else 1
            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1,dp[i - 1][j - 1] + cost)  # Substitution

    # Return the minimum edit distance
    return dp[m][n]

# Test with example strings
source_strings = ["intention", "Piece"]
target_strings = ["execution", "Peace"]

for source, target in zip(source_strings, target_strings):
    distance = min_edit_distance(source, target)
    print(f"Source String: {source}")
    print(f"Target String: {target}")
    print(f"Minimum Edit Distance: {distance}")
    print()

#12---------------------------
import nltk

from textblob import TextBlob
sampleSentence=''' Learning NLP equips individuals good with skills
to analyze vast amounts of textual data, build intelligent chatbots,
automate language-related tasks, and contribute to groundbreaking
advancements in fields like artificial intelligence and linguistics'''
sampleSentence = nltk.word_tokenize(sampleSentence)
sampleSentencePOS = nltk.pos_tag(sampleSentence)
#Find Nouns or proper Nouns
OnlyNouns = (" ").join([POStags[0] for POStags in sampleSentencePOS if POStags[1] in ['NN','NNP']])
# Find only Adjectives
OnlyAdjectives= (" ").join([POStags[0] for POStags in sampleSentencePOS if POStags[1] in ['JJ','JJR','JJS']])
print ('Nouns: ', OnlyNouns)
print ('Adjectives: ', OnlyAdjectives)
print('Overall sentiment score of ajdectives:',TextBlob(OnlyAdjectives).sentiment)

