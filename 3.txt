# Import the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
import string

# Set plotting style
sns.set_style("darkgrid")  # darkgrid, whitegrid, dark, white, and ticks
%matplotlib inline
messages = pd.read_csv('spam.csv', encoding='latin-1')
messages = messages.drop(labels=["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)
messages.columns = ["label", "message"]
print(messages.info())
print(messages.describe())
print(messages.groupby('label').describe().T)
messages["label"].value_counts().plot(kind='pie', explode=[0, 0.1], figsize=(6, 6),
                                      autopct='%1.1f%%', shadow=True)
plt.title("Spam vs Ham")
plt.legend(["Ham", "Spam"])
plt.show()
def text_preprocess(mess):
    
    nopunc = [char for char in mess if char not in string.punctuation]
    nopunc = ''.join(nopunc)
    nopunc = nopunc.lower()
    nostop = [word for word in nopunc.split() if word.lower() not in stopwords.words('english') and word.isalpha()]
    return nostop

spam_messages = messages[messages["label"] == "spam"]["message"]
ham_messages = messages[messages["label"] == "ham"]["message"]
print("No of spam messages: ", len(spam_messages))
print("No of ham messages: ", len(ham_messages))
spam_words = text_preprocess(spam_messages)
spam_wordcloud = WordCloud(width=600, height=400).generate(' '.join(spam_words))
plt.figure(figsize=(10, 8), facecolor='k')
plt.imshow(spam_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

print("Top 10 Spam words are:\n", pd.Series(spam_words).value_counts().head(10))

ham_words = text_preprocess(ham_messages)
ham_wordcloud = WordCloud(width=600, height=400).generate(' '.join(ham_words))
plt.figure(figsize=(10, 8), facecolor='k')
plt.imshow(ham_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

messages["message"] = messages["message"].apply(text_preprocess)
messages["message"] = messages["message"].agg(lambda x: ' '.join(map(str, x)))

# Creating the Bag of Words
vectorizer = CountVectorizer()
bow_transformer = vectorizer.fit(messages['message'])
print("20 Bag of Words (BOW) Features: \n")
print("\nTotal number of vocab words: ", len(vectorizer.vocabulary_))

message4 = messages['message'][3]
bow4 = bow_transformer.transform([message4])
print(bow4)
print(bow4.shape)

messages_bow = bow_transformer.transform(messages['message'])
print('Shape of Sparse Matrix: ', messages_bow.shape)
print('Amount of Non-Zero occurrences: ', messages_bow.nnz)

from sklearn.feature_extraction.text import TfidfTransformer

# TF-IDF Transformation
tfidf_transformer = TfidfTransformer().fit(messages_bow)
tfidf4 = tfidf_transformer.transform(bow4)
print(tfidf4)

vec = TfidfVectorizer(encoding="latin-1", strip_accents="unicode", stop_words="english")
features = vec.fit_transform(messages["message"])
print(features.shape)
print(len(vec.vocabulary_))
print(tfidf_transformer.idf_[bow_transformer.vocabulary_['say']])

messages_tfidf = tfidf_transformer.transform(messages_bow)
print(messages_tfidf.shape)

# Model Evaluation
msg_train, msg_test, label_train, label_test = train_test_split(messages_tfidf, messages['label'], test_size=0.2)
print("train dataset features size: ", msg_train.shape)
print("train dataset label size", label_train.shape)
print("\n")
print("test dataset features size", msg_test.shape)
print("test dataset label size", label_test.shape)

# Building Naive Bayes classifier Model
clf = MultinomialNB()
spam_detect_model = clf.fit(msg_train, label_train)
predict_train = spam_detect_model.predict(msg_train)

print("Classification Report:\n", metrics.classification_report(label_train, predict_train))
print("\nConfusion Matrix:\n", metrics.confusion_matrix(label_train, predict_train))
print("\nAccuracy of Train dataset: {0:0.3f}".format(metrics.accuracy_score(label_train, predict_train)))

# Model Evaluation
label_predictions = spam_detect_model.predict(msg_test)
print(label_predictions)
print('predicted:', spam_detect_model.predict(tfidf4)[0])
print('expected:', messages['label'][3])

# Printing the Overall Accuracy of the model
print("Accuracy of the model: {0:0.3f}".format(metrics.accuracy_score(label_test, label_predictions)))








13
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
import warnings
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from wordcloud import WordCloud
from matplotlib import style
from textblob import TextBlob

%matplotlib inline
warnings.filterwarnings('ignore')

style.use('ggplot')

# Loading the data
df = pd.read_csv('vaccination_tweets.csv')
df.head()
df.info()
df.isnull().sum()

df.columns

text_df = df.drop(['id', 'user_name', 'user_location',
                   'user_description', 'user_created',
                   'user_followers', 'user_friends', 'user_favourites',
                   'user_verified', 'date', 'hashtags', 'source',
                   'retweets', 'favorites', 'is_retweet'], axis=1)
text_df.head()
print(text_df['text'].iloc[0],"\n")
print(text_df['text'].iloc[1],"\n")
print(text_df['text'].iloc[2],"\n")
print(text_df['text'].iloc[3],"\n")
print(text_df['text'].iloc[4],"\n")

text_df.info()

stop_words = set(stopwords.words('english'))
def data_processing(text):
    text = text.lower()
    text = re.sub(r"https\S+|www\S+https\S+", '',text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#','',text)
    text = re.sub(r'[^\w\s]','',text)
    text_tokens = word_tokenize(text)
    filtered_text = [w for w in text_tokens if not w in stop_words]
    return " ".join(filtered_text)

text_df.text = text_df['text'].apply(data_processing)
text_df = text_df.drop_duplicates('text')
stemmer = PorterStemmer()

def stemming(data):
    text = [stemmer.stem(word) for word in data]
    return data

text_df['text'] = text_df['text'].apply(lambda x: stemming(x))

text_df.head()

print(text_df['text'].iloc[0],"\n")
print(text_df['text'].iloc[1],"\n")
print(text_df['text'].iloc[2],"\n")
print(text_df['text'].iloc[3],"\n")
print(text_df['text'].iloc[4],"\n")

text_df.info()

# Exploratory Data Analysis
def polarity(text):
    return TextBlob(text).sentiment.polarity

text_df['polarity'] = text_df['text'].apply(polarity)
text_df.head(10)

def sentiment(label):
    if label < 0:
        return "Negative"
    elif label == 0:
        return "Neutral"
    elif label > 0:
        return "Positive"

text_df['sentiment'] = text_df['polarity'].apply(sentiment)
text_df.head()

fig = plt.figure(figsize=(5, 5))
sns.countplot(x='sentiment', data=text_df)

fig = plt.figure(figsize=(7, 7))
colors = ("yellowgreen", "gold", "red")
wp = {'linewidth': 2, 'edgecolor': "black"}
tags = text_df['sentiment'].value_counts()
explode = (0.1, 0.1, 0.1)
tags.plot(kind='pie', autopct='%1.1f%%', shadow=True,
          colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='')
plt.title('Distribution of sentiments')

pos_tweets = text_df[text_df.sentiment == 'Positive']
pos_tweets = pos_tweets.sort_values(['polarity'], ascending=False)
pos_tweets.head()

text = ' '.join([word for word in pos_tweets['text']])
plt.figure(figsize=(20, 15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in positive tweets', fontsize=19)
plt.show()

neg_tweets = text_df[text_df.sentiment == 'Negative']
neg_tweets = neg_tweets.sort_values(['polarity'], ascending=False)
neg_tweets.head()

text = ' '.join([word for word in neg_tweets['text']])
plt.figure(figsize=(20, 15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in negative tweets', fontsize=19)
plt.show()

neutral_tweets = text_df[text_df.sentiment == 'Neutral']
neutral_tweets = neutral_tweets.sort_values(['polarity'], ascending=False)
neutral_tweets.head()

text = ' '.join([word for word in neutral_tweets['text']])
plt.figure(figsize=(20, 15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in neutral tweets', fontsize=19)
plt.show()

# ML model:
vect = CountVectorizer(ngram_range=(1, 2)).fit(text_df['text'])
X = text_df['text']
Y = text_df['sentiment']
X = vect.transform(X)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("Size of x_train:", (x_train.shape))
print("Size of y_train:", (y_train.shape))
print("Size of x_test:", (x_test.shape))
print("Size of y_test:", (y_test.shape))

logreg = LogisticRegression()
logreg.fit(x_train, y_train)
logreg_pred = logreg.predict(x_test)
logreg_acc = accuracy_score(logreg_pred, y_test)
print("Test accuracy: {:.2f}%".format(logreg_acc * 100))

print(confusion_matrix(y_test, logreg_pred))
print("\n")
print(classification_report(y_test, logreg_pred))

style.use('classic')
cm = confusion_matrix(y_test, logreg_pred, labels=logreg.classes_)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)
disp.plot()
plt.show()




